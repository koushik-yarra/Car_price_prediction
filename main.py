# -*- coding: utf-8 -*-
"""Car price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zAaV7UumggsQsYqz_drI3EsiCSrbuDMl
"""

# importing required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

# loading the dataset
df=pd.read_csv('car.csv')

df

# checking the null values
df.isnull().sum()

df.head(5)

df.columns

df.info()

# unique car names
df['Car_Name'].unique()

# checking the data having any duplicates
df.duplicated().sum()

# dropping the duplicates
df.drop_duplicates(inplace=True)

df.duplicated().sum()

# checking the types of fuels that vehicles contains
print(df.Fuel_Type.value_counts())

# seller type
print(df.Seller_Type.value_counts())

# checking the type transmission
print(df.Transmission.value_counts())

# replacing the categorical values with numerical values
df.replace({'Fuel_Type':{'Petrol':0,'Diesel':1,'CNG':2}},inplace=True)
df.replace({'Seller_Type':{'Dealer':0,'Individual':1}},inplace=True)
df.replace({'Transmission':{'Manual':0,'Automatic':1}},inplace=True)
df.head()

from sklearn.preprocessing import LabelEncoder

# Encode Car Name
label_encoder = LabelEncoder()
df['Car_Name'] = label_encoder.fit_transform(df['Car_Name'])  # Convert to numerical values

# Save the Label Encoder for later use in Streamlit
import pickle
pickle.dump(label_encoder, open('label_encoder.sav', 'wb'))

x=df.drop(['Selling_Price'],axis=1)
y=df['Selling_Price']

# splitting the data
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=2)

# getting inear regression model
model=LinearRegression()

# standardizing the columns
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

model.fit(x_train,y_train)

# prediting the trainind data
pred=model.predict(x_train)

# error score of training data
error_score=r2_score(y_train,pred)

error_score

# plotting a scatter plot between actual and predicted price of training data
plt.scatter(y_train,pred)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Price')
plt.show()

# predicting the testing data
test_pred=model.predict(x_test)

# error score of testing data
error_score_test=r2_score(y_test,test_pred)

error_score_test

"""R² score of 87% (0.87) for training data: Your model explains 87% of the variance in y_train using X_train. This suggests a strong fit on the training data.

R² score of 85% (0.85) for test data: Your model explains 85% of the variance in y_test using X_test. This is still a strong score and close to the training score.

Since your test score is close to the training score, your model generalizes well to unseen data, meaning it is not overfitting.
"""

#plotting a graph between actual and predicted price of testing data
plt.scatter(y_test,test_pred)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Price')
plt.show()

mae=mean_absolute_error(y_test,test_pred)
mse=mean_squared_error(y_test,test_pred)
rmse=np.sqrt(mse)
print('MAE:',mae)
print('MSE:',mse)
print('RMSE:',rmse)

#implementing the lasso regression model
lasso=Lasso()
lasso.fit(x_train,y_train)

# predicting training and testing data
training_pred=lasso.predict(x_train)
testing_pred=lasso.predict(x_test)

error_lasso=r2_score(y_train,training_pred)
error_lasso_test=r2_score(y_test,testing_pred)
print('R2 score for training data:',error_lasso)
print('R2 score for testing data:',error_lasso_test)

"""Training R² = 0.768 → Your model explains about 76.8% of the variance in the training data.

Testing R² = 0.666 → Your model explains about 66.6% of the variance in the test data.

Your model captures a significant portion of the variance but still leaves 23.2% unexplained in training and 33.4% unexplained in testing.

so better go with another model

"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Initialize the RandomForestRegressor
rf_model = RandomForestRegressor(random_state=2)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}


# Perform GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(x_train, y_train)


# Get the best model from GridSearchCV
best_rf_model = grid_search.best_estimator_

# Make predictions on the test set using the best model
test_pred = best_rf_model.predict(x_test)


# Evaluate the model
error_score_test = r2_score(y_test, test_pred)
mae = mean_absolute_error(y_test, test_pred)
mse = mean_squared_error(y_test, test_pred)
rmse = np.sqrt(mse)

print("R2 Score (Test):", error_score_test)
print("MAE:", mae)
print("MSE:", mse)
print("RMSE:", rmse)
print('best parameteres :', best_rf_model)

# Plot Actual vs Predicted values for the test set
plt.scatter(y_test, test_pred)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted Price (Random Forest)")
plt.show()

"""Key Insights:

R² Score (Test) = 0.974 (~97.4%)
✅ Your model explains 97.4% of the variance in the test data, which is an excellent fit!
✅ The model generalizes well to unseen data.

Mean Absolute Error (MAE) = 0.441
✅ On average, your predictions are off by only ~0.44 units, which is quite low.

Mean Squared Error (MSE) = 0.430
✅ The average squared error is much lower than before, indicating improved accuracy.

Root Mean Squared Error (RMSE) = 0.656
✅ The typical prediction error is about 0.66 units, much smaller than your previous RMSE of 1.54.
✅ Since RMSE penalizes large errors more than MAE, this suggests fewer extreme errors.
"""

# Testing with new data
import numpy as np
import pandas as pd

new_data = pd.DataFrame({
    'Car_Name': [0],
    'Year': [2018],
    'Present_Price': [5.59],
    'Kms_Driven': [27000],
    'Fuel_Type': [0],  # Assuming 0 represents 'Petrol'
    'Seller_Type': [0], # Assuming 0 represents 'Dealer'
    'Transmission': [1], # Assuming 1 represents 'Automatic'
    'Owner': [0],
})

# Preprocess the new data
new_data_scaled = sc.transform(new_data)

# Make predictions using the best RandomForestRegressor model
new_prediction = best_rf_model.predict(new_data_scaled)

print("Predicted Selling Price for the new car:", new_prediction[0])

# saving the model

import pickle

filename = 'finalized_model.sav'
pickle.dump(best_rf_model, open(filename, 'wb'))

import pickle

# Save the fitted StandardScaler
scaler_filename = 'scaler.sav'
pickle.dump(sc, open(scaler_filename, 'wb'))

